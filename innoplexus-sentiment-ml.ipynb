{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "839fc1317e1b7253241839bbfa2d40303c53a3f1"
   },
   "source": [
    "## General information\n",
    "\n",
    "In this kernel I'll work with data from Movie Review Sentiment Analysis Playground Competition.\n",
    "\n",
    "This dataset is interesting for NLP researching. Sentences from original dataset were split in separate phrases and each of them has a sentiment label. Also a lot of phrases are really short which makes classifying them quite challenging. Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "pd.set_option('max_colwidth',400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "import datetime\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', sep=\"\\t\")\n",
    "# test = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', sep=\"\\t\")\n",
    "# sub = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "sub = pd.read_csv('../input/sample_submission.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "f9b8d8423bb09068cb168b67f4756ee8b250fc8c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0</td>\n",
       "      <td>Autoimmune diseases tend to come in clusters. As for Gilenya – if you feel good, don’t think about it, it won’t change anything but waste your time and energy. I’m taking Tysabri and feel amazing, no symptoms (other than dodgy color vision, but I’ve had it since always, so, don’t know) and I don’t know if it will last a month, a year, a decade, ive just decided to enjoy the ride, no point in w...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9eba8f80e7e20f3a2f48685530748fbfa95943e4</td>\n",
       "      <td>I can completely understand why you’d want to try it. But, results reported in lectures don’t always stand up to the scrutiny of peer-review during publication. There so much still to do before this is convincing. I hope that it does work out, I really do. And if you’re aware of and happy with the risks, then that’s great. I just think it’s important to present this in a balanced way, and to u...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fe809672251f6bd0d986e00380f48d047c7e7b76</td>\n",
       "      <td>Interesting that it only targets S1P-1/5 receptors rather than 1-5 like Fingolimod. Hoping to soon see what the AEs and SAEs were Yes. I'm not sure what this means, exactly:  Quote Nine patients reported serious adverse events (2 mg: 3/29 [10.3%], 1.25 mg: 1/43 [2.3%], 0.5 mg: 4/29 [13.8%], and 0.25 mg: 1/50 [2.0%]; no serious adverse event was reported for more than 1 patient and no new safet...</td>\n",
       "      <td>fingolimod</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bd22104dfa9ec80db4099523e03fae7a52735eb6</td>\n",
       "      <td>Very interesting, grand merci. Now I wonder where lemtrada and ocrevus sales would go, if they prove anti-cd20 are induction</td>\n",
       "      <td>ocrevus</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b227688381f9b25e5b65109dd00f7f895e838249</td>\n",
       "      <td>Hi everybody, My latest MRI results for Brain and Cervical Cord are in and my next Neurologist appointment is in the next couple of weeks. There’re no new lesions in Brain/Cord and I’ve had no relapses while I was on Gilenya. This was a good sign. But there was one line in the cervical cord review that concerned me. It goes : “Lesions at C2-3 and T2 now show hypointensity on the post gadoliniu...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a043780c757966243779bf3c0d11bf6eef721971</td>\n",
       "      <td>I can’t give you advice about Lemtrada because I chose Cladribine. Have you thought about this drug? The doctors at Barts are keen to give it to people with SPMS. You can read about it here: http://multiple-sclerosis-research.blogspot.com/2016/01/suppose-there-was-therapy-for-all.html</td>\n",
       "      <td>cladribine</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>be5a13376933a7f9bbf8e801c31691092f63260a</td>\n",
       "      <td>Reply posted for JessZidek. Hi Jess Sorry to read about the challenges you are having with your health. You mentioned a lot in your post. I just want to share some info on a few of the points. First, I know you said that you are scared of Humira. Humira and other biologics are very successful in reducing symptoms and inducing and maintain disease remission. To reduce your level of fear it can ...</td>\n",
       "      <td>humira</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>08c3c0c702fc97d290204b37798ac62005da5626</td>\n",
       "      <td>Well as expected my Neurologist wants me to start Tysabri, I kept saying that I wasn’t happy and he kept saying Yes you are! But I am still NOT, If Lemtrada was available here I think I would definitely go for that,but like every thing here we are way behind, it took 8 years longer than Australia to get Gilenya and Tysabri? I am taking Gilenya every second day,but my vitals are still low,white...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8fd3d7ad80791c9343e5cf8a83bd1adf6577d516</td>\n",
       "      <td>Why do you think that FIngolimod was such a miserable failure in progressive MS trial in humans (not animals) that was aborted by Biogen? If it is in fact stimulating neuronal gene expression, axon growth and regeneration, which is what you want in progressive patients, why do human trials fail?</td>\n",
       "      <td>fingolimod</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>793c5af7cc8332df17eb602247d886fbd1c80f89</td>\n",
       "      <td>Thank you so much…I’m learning a lot here at GRACE.  I should have mentioned my husband’s cancer is in his bones, liver, adrenal, in addition to lung and brain mets as I mentioned.   I truly appreciate the comments on hospice as we just started hospice a few weeks ago…my insurance allows palliative care along with continued anti-cancer treatment.  I only thought of hospice as end-of-life care,...</td>\n",
       "      <td>tagrisso</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                unique_hash    ...    sentiment\n",
       "0  2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0    ...            2\n",
       "1  9eba8f80e7e20f3a2f48685530748fbfa95943e4    ...            2\n",
       "2  fe809672251f6bd0d986e00380f48d047c7e7b76    ...            2\n",
       "3  bd22104dfa9ec80db4099523e03fae7a52735eb6    ...            2\n",
       "4  b227688381f9b25e5b65109dd00f7f895e838249    ...            1\n",
       "5  a043780c757966243779bf3c0d11bf6eef721971    ...            2\n",
       "6  be5a13376933a7f9bbf8e801c31691092f63260a    ...            0\n",
       "7  08c3c0c702fc97d290204b37798ac62005da5626    ...            2\n",
       "8  8fd3d7ad80791c9343e5cf8a83bd1adf6577d516    ...            1\n",
       "9  793c5af7cc8332df17eb602247d886fbd1c80f89    ...            2\n",
       "\n",
       "[10 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "4f75e24b86e3aeb7477fa1cd69789992233420b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average count of text per drug in train is 52.\n",
      "Average count of text per drug in test is 31.\n"
     ]
    }
   ],
   "source": [
    "print('Average count of text per drug in train is {0:.0f}.'.format(train.groupby('drug')['text'].count().mean()))\n",
    "print('Average count of text per drug in test is {0:.0f}.'.format(test.groupby('drug')['text'].count().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "ca3148a6bbdd0f71e4909feb5dca874fa6d64a2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text in train: 5279. Number of drug in train: 102.\n",
      "Number of text in test: 2924. Number of drug in test: 95.\n"
     ]
    }
   ],
   "source": [
    "print('Number of text in train: {}. Number of drug in train: {}.'.format(train.shape[0], len(train.drug.unique())))\n",
    "print('Number of text in test: {}. Number of drug in test: {}.'.format(test.shape[0], len(test.drug.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "bef53d8f659b78e3fc6b1ed07025591d55b7c128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length of text in train is 341.\n",
      "Average word length of text in test is 397.\n"
     ]
    }
   ],
   "source": [
    "print('Average word length of text in train is {0:.0f}.'.format(np.mean(train['text'].apply(lambda x: len(x.split())))))\n",
    "print('Average word length of text in test is {0:.0f}.'.format(np.mean(test['text'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    3825\n",
       "1     837\n",
       "0     617\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9ee27697b41dfdca9cdb482bfc85cfd3d63ae6e2"
   },
   "source": [
    "We can see than sentences were split in 18-20 phrases at average and a lot of phrases contain each other. Sometimes one word or even one punctuation mark influences the sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d1efbed65f250d37472544f4fe37cb6fd13e183",
    "collapsed": true
   },
   "source": [
    "Let's see for example most common trigrams for positive phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "4a128ab1d36eef6ec47161705a2b1094b830fe10"
   },
   "outputs": [],
   "source": [
    "text = ' '.join(train.loc[train.sentiment == 0, 'text'].values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "b5c8533c5861794d44b53dbb2e5ef764e5e88119"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('2016', '(HealthDay', 'News)', '--'), 138),\n",
       " (('(subscription', 'or', 'payment', 'may'), 112),\n",
       " (('or', 'payment', 'may', 'be'), 112),\n",
       " (('payment', 'may', 'be', 'required)'), 112),\n",
       " (('according', 'to', 'a', 'study'), 90),\n",
       " (('to', 'a', 'study', 'published'), 90),\n",
       " (('Full', 'Text', '(subscription', 'or'), 84),\n",
       " (('Text', '(subscription', 'or', 'payment'), 84),\n",
       " (('.', 'Full', 'Text', '(subscription'), 78),\n",
       " (('a', 'study', 'published', 'online'), 65),\n",
       " (('for', 'the', 'treatment', 'of'), 65),\n",
       " (('study', 'published', 'online', 'Dec.'), 57),\n",
       " (('(HealthDay', 'News)', '--', 'For'), 33),\n",
       " (('in', 'the', 'Journal', 'of'), 31),\n",
       " (('Food', 'and', 'Drug', 'Administration'), 31),\n",
       " (('Editorial', '(subscription', 'or', 'payment'), 26),\n",
       " (('Medicine', '.', 'Full', 'Text'), 23),\n",
       " (('a', 'study', 'published', 'in'), 23),\n",
       " (('may', 'be', 'required)', 'Editorial'), 23),\n",
       " (('be', 'required)', 'Editorial', '(subscription'), 23),\n",
       " (('required)', 'Editorial', '(subscription', 'or'), 23),\n",
       " (('(HealthDay', 'News)', '--', 'The'), 22),\n",
       " (('published', 'in', 'the', 'Dec.'), 21),\n",
       " (('study', 'published', 'in', 'the'), 20),\n",
       " (('has', 'spread', 'through', 'the'), 20),\n",
       " (('non-small', 'cell', 'lung', 'cancer'), 19),\n",
       " (('and', 'Drug', 'Administration', '(FDA)'), 18),\n",
       " (('of', 'the', 'colon', 'wall'), 18),\n",
       " (('I', 'was', 'diagnosed', 'with'), 17),\n",
       " (('other', 'parts', 'of', 'the'), 17),\n",
       " (('mg', 'every', 'other', 'week'), 17),\n",
       " (('New', 'England', 'Journal', 'of'), 16),\n",
       " (('the', 'Journal', 'of', 'the'), 16),\n",
       " (('U.S.', 'Food', 'and', 'Drug'), 16),\n",
       " (('England', 'Journal', 'of', 'Medicine'), 15),\n",
       " (('Journal', 'of', 'the', 'American'), 15),\n",
       " (('according', 'to', 'research', 'published'), 15),\n",
       " (('in', 'the', 'treatment', 'of'), 15),\n",
       " (('enhance', 'the', 'adverse/toxic', 'effect'), 15),\n",
       " (('the', 'adverse/toxic', 'effect', 'of'), 15),\n",
       " (('News)', '--', 'For', 'patients'), 14),\n",
       " (('the', 'New', 'England', 'Journal'), 14),\n",
       " (('Journal', 'of', 'Medicine', '.'), 14),\n",
       " (('the', 'colon', 'wall', 'to'), 14),\n",
       " (('(HealthDay', 'News)', '--', 'A'), 13),\n",
       " (('I', 'have', 'been', 'on'), 13),\n",
       " (('treatment', 'of', 'patients', 'with'), 13),\n",
       " (('layer)', 'of', 'the', 'colon'), 13),\n",
       " (('has', 'been', 'shown', 'to'), 13),\n",
       " (('[Epub', 'ahead', 'of', 'print]'), 13)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(text_trigrams).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('(HealthDay', 'News)', '--'), 139),\n",
       " (('2016', '(HealthDay', 'News)'), 138),\n",
       " (('.', 'Full', 'Text'), 114),\n",
       " (('(subscription', 'payment', 'may'), 112),\n",
       " (('payment', 'may', 'required)'), 112),\n",
       " (('according', 'study', 'published'), 90),\n",
       " (('Full', 'Text', '(subscription'), 84),\n",
       " (('Text', '(subscription', 'payment'), 84),\n",
       " (('published', 'online', 'Dec.'), 80),\n",
       " (('study', 'published', 'online'), 67),\n",
       " (('News)', '--', 'For'), 33),\n",
       " (('Food', 'Drug', 'Administration'), 31),\n",
       " (('cell', 'lung', 'cancer'), 30),\n",
       " (('Editorial', '(subscription', 'payment'), 26),\n",
       " (('non-small', 'cell', 'lung'), 25),\n",
       " (('Medicine', '.', 'Full'), 23),\n",
       " (('may', 'required)', 'Editorial'), 23),\n",
       " (('required)', 'Editorial', '(subscription'), 23),\n",
       " (('News)', '--', 'The'), 22),\n",
       " (('mg', 'every', 'week'), 22),\n",
       " (('Drug', 'Administration', '(FDA)'), 18),\n",
       " (('primary', 'progressive', 'MS'), 17),\n",
       " (('40', 'mg', 'every'), 17),\n",
       " (('New', 'England', 'Journal'), 16),\n",
       " (('U.S.', 'Food', 'Drug'), 16),\n",
       " (('England', 'Journal', 'Medicine'), 15),\n",
       " (('according', 'research', 'published'), 15),\n",
       " (('high-dose', 'vitamin', 'C'), 15),\n",
       " (('enhance', 'adverse/toxic', 'effect'), 15),\n",
       " (('median', 'overall', 'survival'), 15)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(train.loc[train.sentiment == 0, 'text'].values)\n",
    "text = [i for i in text.split() if i not in stopwords.words('english')]\n",
    "text_trigrams = [i for i in ngrams(text, 3)]\n",
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "402c0b58fe43a680bea33e1813012bec5c16cb55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((\"Crohn's\", '&', 'Colitis'), 30),\n",
       " (('I', 'feel', 'like'), 29),\n",
       " (('I', 'think', 'I'), 19),\n",
       " (('bad', 'sometimes', 'deadly'), 17),\n",
       " (('face,', 'lips,', 'tongue,'), 17),\n",
       " (('doctor', 'right', 'away'), 17),\n",
       " (('I', 'know', 'I'), 17),\n",
       " (('I', 'don’t', 'know'), 16),\n",
       " (('report', 'side', 'effects'), 16),\n",
       " (('&', 'Colitis', 'Community'), 16),\n",
       " (('natural', 'products,', 'vitamins'), 15),\n",
       " (('side', 'effects', 'may'), 15),\n",
       " (('Recommended', 'safety', 'management'), 14),\n",
       " (('safety', 'management', 'strategies'), 14),\n",
       " (('management', 'strategies', 'recommended'), 14),\n",
       " (('This', 'medicine', 'may'), 13),\n",
       " (('Call', 'doctor', 'right'), 13),\n",
       " (('side', 'effects.', 'You'), 13),\n",
       " (('Prescribing', 'Information', 'Warnings:'), 13),\n",
       " (('Lung', 'Cancer', ','), 13),\n",
       " (('This', 'topic', 'modified'), 12),\n",
       " (('swelling', 'face,', 'lips,'), 12),\n",
       " (('OTC,', 'natural', 'products,'), 12),\n",
       " (('lips,', 'tongue,', 'throat.'), 12),\n",
       " (('effects.', 'You', 'may'), 12),\n",
       " (('strategies', 'recommended', 'monitoring:'), 12),\n",
       " (('white', 'blood', 'cell'), 11),\n",
       " (('What', 'side', 'effects'), 11),\n",
       " (('call', 'doctor', 'right'), 11),\n",
       " (('Call', 'doctor', 'medical'), 11)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(train.loc[train.sentiment == 1, 'text'].values)\n",
    "text = [i for i in text.split() if i not in stopwords.words('english')]\n",
    "text_trigrams = [i for i in ngrams(text, 3)]\n",
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('cell', 'lung', 'cancer'), 333),\n",
       " (('non-small', 'cell', 'lung'), 308),\n",
       " (('(Opens', 'new', 'window)'), 192),\n",
       " (('small', 'cell', 'lung'), 157),\n",
       " (('new', 'window)', 'Click'), 154),\n",
       " (('Cell', 'Lung', 'Cancer'), 138),\n",
       " ((',', \"Crohn's\", 'Disease'), 129),\n",
       " (('enhance', 'adverse/toxic', 'effect'), 124),\n",
       " (('cell', 'lung', 'cancer.'), 115),\n",
       " (('PM', 'Ulcerative', 'Colitis'), 112),\n",
       " (('(', 'NCI', 'Thesaurus'), 110),\n",
       " (('NCI', 'Thesaurus', ')'), 110),\n",
       " (('active', 'clinical', 'trials'), 109),\n",
       " (('clinical', 'trials', 'using'), 109),\n",
       " (('Check', 'active', 'clinical'), 108),\n",
       " (('trials', 'using', 'agent.'), 108),\n",
       " (('using', 'agent.', '('), 108),\n",
       " (('agent.', '(', 'NCI'), 108),\n",
       " (('Member', 'Joined', ':'), 102),\n",
       " ((',', 'Ulcerative', 'Colitis'), 93),\n",
       " (('Non-Small', 'Cell', 'Lung'), 92),\n",
       " (('doctor', 'right', 'away'), 91),\n",
       " (('every', '8', 'weeks'), 91),\n",
       " (('growth', 'factor', 'receptor'), 86),\n",
       " (('A', 'clinical', 'trial'), 85),\n",
       " (('lung', 'cancer', '(NSCLC)'), 83),\n",
       " (('Food', 'Drug', 'Administration'), 80),\n",
       " (('window)', 'Click', 'share'), 80),\n",
       " (('may', 'enhance', 'adverse/toxic'), 79),\n",
       " (('Moderator', 'Ulcerative', 'Colitis'), 78)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(train.loc[train.sentiment == 2, 'text'].values)\n",
    "text = [i for i in text.split() if i not in stopwords.words('english')]\n",
    "text_trigrams = [i for i in ngrams(text, 3)]\n",
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f59db6ed32fafb024728fd95b96157f278682a74"
   },
   "source": [
    "The results show the main problem with this dataset: there are to many common words due to sentenced splitted in phrases. As a result stopwords shouldn't be removed from text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10dc8fc8d535ef492a3dab1b85b4052feec756ee"
   },
   "source": [
    "### Thoughts on feature processing and engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5d018a694d10cbd9e8e89c26d5227fdb9cf8c0b"
   },
   "source": [
    "So, we have only phrases as data. And a phrase can contain a single word. And one punctuation mark can cause phrase to receive a different sentiment. Also assigned sentiments can be strange. This means several things:\n",
    "- using stopwords can be a bad idea, especially when phrases contain one single stopword;\n",
    "- puntuation could be important, so it should be used;\n",
    "- ngrams are necessary to get the most info from data;\n",
    "- using features like word count or sentence length won't be useful;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "bd603ad818970c3c8c6db5e430a6cb8ae8eafbd5"
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "train['text_orig'] = train['text']\n",
    "test['text_orig'] = test['text']\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "train['text'] = train['text_orig'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "test['text'] = test['text_orig'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "\n",
    "# train['text'] = train['text_orig'].apply(lambda x: [item for item in x if item not in stop])\n",
    "# test['text'] = test['text_orig'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #remove stop words\n",
    "# test_filt = test[0:10].copy()\n",
    "# test_filt['text_orig'] = test_filt['text']\n",
    "\n",
    "# stop = stopwords.words('english')\n",
    "\n",
    "# test_filt['text'] = test_filt['text_orig'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "# # test['text'] = test['text_orig'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "# # train['text'] = [i for i in train['text_orig'].str.split() if i not in stopwords.words('english')]\n",
    "# # test['text'] = [i for i in test['text_orig'].str.split() if i not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2919    Reply posted for Hippopostrous. We are sorry to read about your daughter’s potential Crohn’s Disease diagnosis. Crohn’s disease can be managed and many patients live well with the disease. It is important to learn all you can about the disease. A good place to start is with A Guide for Parents brochure http://www.crohnscolitisfoundation.org/assets/pdfs/parent_guide_final_052016.pdf and Underst...\n",
      "2920    Aw Lorraine That's crap. What a shame you've had to stop Tysabri. I'm not in exactly the same position as you, but not currently on a DMD either. Tysabri gave me raised liver enzymes so I had to stop taking it. Then Tecfidera (which I presume is the other drug you can't take because of your JCV status) gave me depleted lymphocytes so I had to stop that too. I've already had bad side effects fr...\n",
      "2921    jskozio14\\n That sounds like nonsense to me.  Experimental?  Baloney!\\n In March of 2105 Opdivo (Nivolumab) was FDA approved for use against squamous NSCLC and later in the year (maybe October) against Adenocaricnoma NSCLC.  Sounds like your insurance company doesn't like the fact that you've got cancer and this drug is working for you.  Yes, it is expensoive but there is nothing else except m...\n",
      "2922    It sounds like you're doing very well FG! I wouldn't be worried that you aren't in \"complete remission\" yet--you are headed in the right direction and are currently in a \"pseudo-remission\" and that is great! Hope you can get rid of these headaches though. 37yr female, dx UC '04, dx changed Crohn's Colitis '17 Currently: Stelara, Apriso, Prednisone *weaning* Previously used: Remicade, Humira, I...\n",
      "2923    Hi @rambles too! It’s such a hard decision, isn’t it? My neurologist said he’d recommend cladribine over gilenya but said lemtrada changes your immune system forever… The MS nurse just said I need to decide how much risk I’m willing to take. I’d already been considering asking for lemtrada as I hadn’t felt that great on tecfidera, I hadn’t heard about cladribine. I know the Bart’s team are rea...\n",
      "Name: text_orig, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2919    Reply posted Hippopostrous. We sorry read daughter’s potential Crohn’s Disease diagnosis. Crohn’s disease managed many patients live well disease. It important learn disease. A good place start A Guide Parents brochure http://www.crohnscolitisfoundation.org/assets/pdfs/parent_guide_final_052016.pdf Understanding Medication brochure http://www.crohnscolitisfoundation.org/assets/pdfs/understandi...\n",
       "2920    Aw Lorraine That's crap. What shame stop Tysabri. I'm exactly position you, currently DMD either. Tysabri gave raised liver enzymes I stop taking it. Then Tecfidera (which I presume drug can't take JCV status) gave depleted lymphocytes I stop too. I've already bad side effects Avonex failed copaxone (ie started relapsing again). Given history, fact I'm disabled MS 19 years, I think another DMD...\n",
       "2921    jskozio14 That sounds like nonsense me. Experimental? Baloney! In March 2105 Opdivo (Nivolumab) FDA approved use squamous NSCLC later year (maybe October) Adenocaricnoma NSCLC. Sounds like insurance company like fact got cancer drug working you. Yes, expensoive nothing else except maybe Keytruda Merck would work similar way also expensive. I would fight anyway everyway can. Several suggestions...\n",
       "2922    It sounds like well FG! I worried \"complete remission\" yet--you headed right direction currently \"pseudo-remission\" great! Hope get rid headaches though. 37yr female, dx UC '04, dx changed Crohn's Colitis '17 Currently: Stelara, Apriso, Prednisone *weaning* Previously used: Remicade, Humira, Imuran, 6MP, Asacol, Lialda, Delzicol, Pentasa, Cortenema, Cortifoam, Rowasa, Canasa, Entocort, Uceris,...\n",
       "2923    Hi @rambles too! It’s hard decision, isn’t it? My neurologist said he’d recommend cladribine gilenya said lemtrada changes immune system forever… The MS nurse said I need decide much risk I’m willing take. I’d already considering asking lemtrada I hadn’t felt great tecfidera, I hadn’t heard cladribine. I know Bart’s team really enthusiastic cladribine I’ve read doesn’t seem effective tecfidera...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test['text_orig'].tail())\n",
    "test['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "7ebc937fd5ec811bc5c529ff416180fe338d073d"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), tokenizer=tokenizer.tokenize)\n",
    "full_text = list(train['text'].values) + list(test['text'].values)\n",
    "vectorizer.fit(full_text)\n",
    "train_vectorized = vectorizer.transform(train['text'])\n",
    "test_vectorized = vectorizer.transform(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x1944141 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 610 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectorized[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "3d0bb4539b1e0b5f8439878a967ce7b5ece23f60"
   },
   "outputs": [],
   "source": [
    "y = train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "75a7acfd815fb391cad92eed8d2f13e5c9801de8"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "ovr = OneVsRestClassifier(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "5a0bf05b880e0d05da378b753394e5a631753e82"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 128 ms, total: 12.1 s\n",
      "Wall time: 9.02 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='warn',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='warn', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ovr.fit(train_vectorized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "5946485410c25ae5033bf39e29f49f606ea87bfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 72.46%, std 0.07.\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(ovr, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=5)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "5946485410c25ae5033bf39e29f49f606ea87bfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean f1 score 28.01%, std 0.02.\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(ovr, train_vectorized, y, scoring='f1_macro', n_jobs=-1, cv=5)\n",
    "print('Cross-validation mean f1 score {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "755c2b04f7bf8e86e9dc6242f392a575c61a052c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 72.65%, std 0.32.\n",
      "CPU times: user 32 ms, sys: 24 ms, total: 56 ms\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc = LinearSVC(dual=False)\n",
    "scores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "55a5845de3412244cfa9f0a11392e50c76d78184",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ovr.fit(train_vectorized, y);\n",
    "svc.fit(train_vectorized, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def runlgb(ispermutefeats,train,test,target,param,cur_features,score_function=None):\n",
    "\n",
    "    \n",
    "    oof = np.zeros((train.shape[0],3))\n",
    "    predictions = np.zeros((test.shape[0],3))\n",
    "    start = time.time()\n",
    "    valid_scores =[]\n",
    "\n",
    "    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=4590)\n",
    "    indices = folds.split(train, target.values)\n",
    "        \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(indices):\n",
    "        print()\n",
    "        print(\"fold n°{}\".format(fold_))\n",
    "\n",
    "        tr = train[trn_idx]\n",
    "        val = train[val_idx]\n",
    "        y_val = target.iloc[val_idx]\n",
    "        y_tr = target.iloc[trn_idx]\n",
    "        \n",
    "        trn_data = lgb.Dataset(tr, label=y_tr)#,, categorical_feature=categorical_feats)\n",
    "        val_data = lgb.Dataset(val, label=y_val)#,, categorical_feature=categorical_feats)\n",
    "        \n",
    "        num_round = param['n_estimators']\n",
    "        clf = lgb.train(param, trn_data, num_round, valid_sets = [val_data], verbose_eval=100, \n",
    "                        early_stopping_rounds = 200)\n",
    "\n",
    "\n",
    "        oof[val_idx,:] = clf.predict(val, num_iteration=clf.best_iteration)\n",
    "\n",
    "        valid_scores+=[clf.best_score['valid_0'][param['metric']]]\n",
    "        predictions += clf.predict(test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "    print('valid scores log loss:',valid_scores)\n",
    "#     pred_labels = oof.reshape(3,-1).argmax(axis=0)\n",
    "# #     pred_labels = oof.reshape(len(np.unique(target)),-1).argmax(axis=0)\n",
    "#     print(oof[0:10,:])\n",
    "#     print(pred_labels[0:10])\n",
    "#     print(oof.reshape(3,-1)[0:10,:])\n",
    "#     print(oof.reshape(-1,3)[0:10,:])\n",
    "    \n",
    "    factors  =[0.1304,0.177,0.8088]\n",
    "    oof_factored =  oof / factors\n",
    "    oof_labels = oof_factored.argmax(axis=1)\n",
    "    print(oof_factored[0:10,:])\n",
    "    print(oof_labels[0:10])\n",
    "    \n",
    "    print(\"CV F1 score: {:<8.5f}\".format(f1_score(target, oof_labels,average='macro')))\n",
    "\n",
    "    predictions_factored =  predictions / factors\n",
    "    test_labels = predictions_factored.argmax(axis=1)\n",
    "    print()\n",
    "    print(\"TEST METRICS\")\n",
    "    print(predictions_factored[0:10,:])\n",
    "    print(test_labels[0:10])\n",
    "    \n",
    "    return predictions,oof,oof_labels,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'multiclass',\n",
    "         'num_class' : 3,\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.1,\n",
    "         \"min_child_samples\": 20,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'multi_logloss',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"nthread\": 4,\n",
    "         'n_estimators' : 1000,\n",
    "         \"random_state\": 4590}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold n°0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.729865\n",
      "[200]\tvalid_0's multi_logloss: 0.828237\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid_0's multi_logloss: 0.700092\n",
      "\n",
      "fold n°1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.752534\n",
      "[200]\tvalid_0's multi_logloss: 0.858207\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's multi_logloss: 0.711047\n",
      "\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.701654\n",
      "[200]\tvalid_0's multi_logloss: 0.792456\n",
      "Early stopping, best iteration is:\n",
      "[51]\tvalid_0's multi_logloss: 0.678314\n",
      "\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.737587\n",
      "[200]\tvalid_0's multi_logloss: 0.833955\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's multi_logloss: 0.700706\n",
      "\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.707425\n",
      "[200]\tvalid_0's multi_logloss: 0.791006\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid_0's multi_logloss: 0.683885\n",
      "valid scores log loss: [0.7000923522264627, 0.7110465281330614, 0.6783144785929593, 0.7007061815259524, 0.6838846309550252]\n",
      "[[0.81089527 0.99285016 0.88838375]\n",
      " [0.91896611 0.9571246  0.87877815]\n",
      " [0.34128361 0.83364035 0.99893951]\n",
      " [0.68761172 0.85578299 0.93825648]\n",
      " [0.54660282 1.97497341 0.71606417]\n",
      " [0.51151037 0.25688623 1.09771289]\n",
      " [0.5570548  0.52005959 1.03277634]\n",
      " [0.32082041 3.695748   0.37588727]\n",
      " [0.17128236 0.95151436 1.00055235]\n",
      " [0.30973729 1.24630902 0.91371607]]\n",
      "[1 1 2 2 1 2 2 1 2 1]\n",
      "CV F1 score: 0.47103 \n",
      "\n",
      "TEST METRICS\n",
      "[[0.54647388 0.84600281 0.96315196]\n",
      " [2.35844658 0.7133132  0.70005209]\n",
      " [0.07190456 0.15813553 1.19019987]\n",
      " [0.21268433 0.10621688 1.17886446]\n",
      " [0.1019841  0.20977509 1.17404931]\n",
      " [2.30332784 0.569659   0.74037637]\n",
      " [0.3919244  0.86369063 0.98419859]\n",
      " [0.88033591 0.47858362 0.98973157]\n",
      " [0.46959172 2.11795954 0.69718892]\n",
      " [1.08117224 1.04232731 0.83398022]]\n",
      "[2 0 2 2 2 0 2 2 1 0]\n",
      "CPU times: user 21min 32s, sys: 11 s, total: 21min 43s\n",
      "Wall time: 5min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_splits=5\n",
    "features =['text']\n",
    "\n",
    "predictions_1,oof_1,pred_labels, test_labels = \\\n",
    "        runlgb(False,train_vectorized,test_vectorized,target,param,features,score_function=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5279,)\n"
     ]
    }
   ],
   "source": [
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(oof_1[0:10,:])\n",
    "# print(oof_1.reshape(3,-1)[0:10,:])\n",
    "# print(oof_1.reshape(3,-1).shape)\n",
    "# pred_labels_new = oof_1.argmax(axis=1)\n",
    "# print(pred_labels_new[1000:1020])\n",
    "# print(\"CV F1 score: {:<8.5f}\".format(f1_score(target, pred_labels_new,average='macro')))\n",
    "\n",
    "# pred_labels_new[pred_labels_new==2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factors  =[0.1304,0.177,0.8088]\n",
    "# oof_2 =  oof_1 / factors\n",
    "# print(oof_2[0:10])\n",
    "\n",
    "# pred_labels_new = oof_2.argmax(axis=1)\n",
    "# print(pred_labels_new[1000:1020])\n",
    "# print(\"CV F1 score: {:<8.5f}\".format(f1_score(target, pred_labels_new,average='macro')))\n",
    "\n",
    "# pred_labels_new[pred_labels_new==2].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['sentiment'] = test_labels\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = cross_val_score(ovr, train_vectorized, y, scoring='f1_macro', n_jobs=-1, cv=5)\n",
    "# print('Cross-validation mean f1 score {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "398461363c7a395e2a982e07e8ac6fccaee139c1"
   },
   "source": [
    "## Deep learning\n",
    "And now let's try DL. DL should work better for text classification with multiple layers. I use an architecture similar to those which were used in toxic competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "eb29ec027df57f6597dbef976645dc8d151e1618",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "# from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "# from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "# from keras.models import Model, load_model\n",
    "# from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "# from keras import backend as K\n",
    "# from keras.engine import InputSpec, Layer\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "# from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "a2881c29f82578b4a373b52d2c7b96a2e73bfd80",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tk = Tokenizer(lower = True, filters='')\n",
    "# tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "1724669c7ca010d1bbf75e200211afdead768d1f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_tokenized = tk.texts_to_sequences(train['Phrase'])\n",
    "# test_tokenized = tk.texts_to_sequences(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "bcb80cf8a59ca779a0be1ab235a1e9da2f4b175b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max_len = 50\n",
    "# X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "# X_test = pad_sequences(test_tokenized, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "9dfd0b8fa2c79bfa206d2fe8e35fbec444418f5c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_path = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "d74c2aa2f13e8544f045e5644d5bac70e248a8bd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "max_features = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "cdb522c23b75331481145b789cf127b39d47eaa1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "# embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
    "\n",
    "# word_index = tk.word_index\n",
    "# nb_words = min(max_features, len(word_index))\n",
    "# embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "# for word, i in word_index.items():\n",
    "#     if i >= max_features: continue\n",
    "#     embedding_vector = embedding_index.get(word)\n",
    "#     if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "365c0d607d55a78c5890268b9c168eb12a211855",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# ohe = OneHotEncoder(sparse=False)\n",
    "# y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "594273c2d887315d083a35a5ffd7c2dd40c2ebb6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "#     file_path = \"best_model.hdf5\"\n",
    "#     check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "#                                   save_best_only = True, mode = \"min\")\n",
    "#     early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "    \n",
    "#     inp = Input(shape = (max_len,))\n",
    "#     x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "#     x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "#     x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "#     x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "#     avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
    "#     max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "#     x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "#     avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
    "#     max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "#     x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "#     x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "#     avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
    "#     max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "#     x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "#     avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
    "#     max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    \n",
    "#     x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n",
    "#                     avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "#     x = Dense(5, activation = \"sigmoid\")(x)\n",
    "#     model = Model(inputs = inp, outputs = x)\n",
    "#     model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "#     history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
    "#                         verbose = 1, callbacks = [check_point, early_stop])\n",
    "#     model = load_model(file_path)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "95f52b1f6de4e939c8d21e3525503912282fbd47"
   },
   "source": [
    "An attempt at ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "5eb586c98fb75c25cac099cd03d8233185fdc317",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model1 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "b059392aad7d904adfb8ae151ad2004aa03da30d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model2 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 128, spatial_dr = 0.5, kernel_size1=3, kernel_size2=2, dense_units=64, dr=0.2, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "8187e167ce93f0eb69f59cb9d7fedc4637a77cfe",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "#     file_path = \"best_model.hdf5\"\n",
    "#     check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "#                                   save_best_only = True, mode = \"min\")\n",
    "#     early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "\n",
    "#     inp = Input(shape = (max_len,))\n",
    "#     x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "#     x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "#     x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "#     x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    \n",
    "#     x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "#     avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
    "#     max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
    "    \n",
    "#     x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "#     avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n",
    "#     max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n",
    "    \n",
    "    \n",
    "#     x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "#     avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n",
    "#     max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n",
    "    \n",
    "#     x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "#     avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n",
    "#     max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n",
    "    \n",
    "    \n",
    "#     x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n",
    "#                     avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "#     x = Dense(5, activation = \"sigmoid\")(x)\n",
    "#     model = Model(inputs = inp, outputs = x)\n",
    "#     model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "#     history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
    "#                         verbose = 1, callbacks = [check_point, early_stop])\n",
    "#     model = load_model(file_path)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "9bf90d6d4effebb3c5aa9b666b8e09c9c57d94d3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model3 = build_model2(lr = 1e-4, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=4, kernel_size2=3, dense_units=32, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "bf6d8d367c5adc30e00bbd77c1de70fd52960441",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model4 = build_model2(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "b10113439be683bf19930750eaf96328e5b58d42",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model5 = build_model2(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.4, conv_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "8529014d1a239f308ac5f2552088ce2d8ebb8966",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pred1 = model1.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "# pred = pred1\n",
    "# pred2 = model2.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "# pred += pred2\n",
    "# pred3 = model3.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "# pred += pred3\n",
    "# pred4 = model4.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "# pred += pred4\n",
    "# pred5 = model5.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "# pred += pred5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "d58a5b52ea647dab51123ef89878c5355b3d2971",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
    "# sub['Sentiment'] = predictions\n",
    "# sub.to_csv(\"blend.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
